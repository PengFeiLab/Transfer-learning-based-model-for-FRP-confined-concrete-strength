{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce52318-56cc-4510-a97d-8195fe1eee1b",
   "metadata": {},
   "source": [
    "# 0 Initialization #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d24fc-09db-42c2-9b0f-4d1b8f2d4c78",
   "metadata": {},
   "source": [
    "## 0.1 Imports and Path ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d883919a-6cde-4468-b740-6e808af9cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Python = 3.10.12\n",
    "import numpy as np  # V1.24.3\n",
    "import pandas as pd # V1.5.3\n",
    "import warnings\n",
    "import matplotlib  # V3.7.1\n",
    "import xgboost  # V1.7.3\n",
    "import shap  # V0.42.1\n",
    "from sklearn.metrics import r2_score as r2s  # V1.3.0\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn import clone\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "from tqdm import trange\n",
    "# <editor-fold desc=\"Protected Imports\">\n",
    "from sklearn.utils import check_random_state, _safe_indexing\n",
    "from sklearn.utils.validation import _num_samples, _check_sample_weight\n",
    "# </editor-fold>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee10dce-d805-4f19-ad61-498b459cef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of data and output\n",
    "path_tar, col_tar = r'C:\\Users\\Administrator\\Desktop\\Open Source\\FRP_Dataset.xls', 'I'\n",
    "path_sou, col_sou = r'C:\\Users\\Administrator\\Desktop\\Open Source\\Steel_Dataset.xls', 'I'\n",
    "save_path = r'C:\\Users\\Administrator\\Desktop\\Open Source\\Results\\Re_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b443a1-d613-41f7-b90b-8db21f8ef25b",
   "metadata": {},
   "source": [
    "## 0.2 Define algorithm classes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e365cff-5f71-442e-8f0b-af68ec0a186f",
   "metadata": {},
   "source": [
    "### 0.2.1 TrAdaBoost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbf1616-e367-408c-a170-3778c28d169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrAdaBoostR:\n",
    "    def __init__(self, learner):\n",
    "        self.learner = learner\n",
    "        self.weight = 0\n",
    "        self.p, self.err, self.beta_t, self.p_train = None, None, None, None\n",
    "        self.learners = []\n",
    "        self.n_iters = 20\n",
    "\n",
    "    def fit(self, x_source: np.ndarray, y_source: np.ndarray, x_target: np.ndarray, y_target: np.ndarray,\n",
    "            n_iters: int = 20):\n",
    "        self.n_iters = n_iters\n",
    "        y_source, y_target = np.ravel(y_source), np.ravel(y_target)\n",
    "        n, m = y_source.shape[0], y_target.shape[0]  # Data Points (diff and same distribution)\n",
    "        x_train = np.concatenate((x_source, x_target), axis=0)\n",
    "        y_train = np.concatenate((y_source, y_target), axis=0)\n",
    "        # 1 Initialization\n",
    "        self.weight = np.zeros((n_iters, n + m))\n",
    "        self.weight[0] = np.ones(n + m) / (n + m)\n",
    "        # 2 Loop\n",
    "        # 2.0 Initialize vector\n",
    "        self.p = np.zeros((n_iters, n + m))\n",
    "        self.p_train = np.zeros((n_iters, n + m))\n",
    "        self.err = np.zeros(n_iters)\n",
    "        self.beta_t = np.zeros(n_iters)\n",
    "        for t in range(n_iters):\n",
    "            # 2.1 get p\n",
    "            self.p[t] = self.weight[t] / self.weight[t].sum()\n",
    "            # 2.2 predict using Learner\n",
    "            learner = clone(self.learner)\n",
    "            learner.fit(x_train, y_train, sample_weight=self.p[t])\n",
    "            self.p_train[t] = learner.predict(x_train)\n",
    "            self.learners.append(learner)\n",
    "            # 2.3 get Learner Loss\n",
    "            loss = self.p_train[t, :] - y_train\n",
    "            l1_same = np.max(np.abs(loss[n:n + m]))\n",
    "            weight_same = self.weight[t, n:n + m].sum()\n",
    "            for i in range(n, n + m):\n",
    "                self.err[t] += abs(self.p_train[t, i] - y_train[i]) * self.weight[t, i] / weight_same / l1_same\n",
    "            # 2.3.1 termination condition\n",
    "            if self.err[t] >= 0.5:\n",
    "                self.p = self.p[:t, :]\n",
    "                self.err = self.err[:t]\n",
    "                self.beta_t = self.beta_t[:t]\n",
    "                self.p_train = self.p_train[:t, :]\n",
    "                break\n",
    "            # 2.4 Get beta\n",
    "            self.beta_t[t] = self.err[t] / (1 - self.err[t])\n",
    "            beta = 1 / (1 + (2 * np.log(n) / n_iters) ** 0.5)\n",
    "            # 2.5 update weight vector\n",
    "            l1_diff = np.max(np.abs(loss[0:n]))\n",
    "            if t == n_iters - 1:\n",
    "                break\n",
    "            for i in range(n):\n",
    "                self.weight[t + 1, i] = self.weight[t, i] * beta ** (abs(self.p_train[t, i] - y_train[i]) / l1_diff)\n",
    "            for i in range(n, n + m):\n",
    "                self.weight[t + 1, i] = self.weight[t, i] * self.beta_t[t] ** \\\n",
    "                                        (- abs(self.p_train[t, i] - y_train[i]) / l1_same)\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # 1 get weight vector by beta_t\n",
    "        weights = self.beta_t / self.beta_t.sum()\n",
    "        # 2 output\n",
    "        predicts = None\n",
    "        for reg in self.learners:\n",
    "            p_x = reg.predict(x)\n",
    "            p_x = p_x.reshape(p_x.shape[0], 1)\n",
    "            if predicts is None:\n",
    "                predicts = p_x\n",
    "            else:\n",
    "                predicts = np.concatenate([predicts, p_x], axis=1)\n",
    "        return predicts.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e4953-9892-472a-ab72-a2801f74928f",
   "metadata": {},
   "source": [
    "### 0.2.2 Two-stage TrAdaboost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291e883b-cb98-4f4d-8491-eab767112b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostR2T(AdaBoostRegressor):\n",
    "    n_protected = 0\n",
    "    sample_weight = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self._validate_params()\n",
    "\n",
    "        X, y = self._validate_data(\n",
    "            X,\n",
    "            y,\n",
    "            accept_sparse=[\"csr\", \"csc\"],\n",
    "            ensure_2d=True,\n",
    "            allow_nd=True,\n",
    "            dtype=None,\n",
    "            y_numeric=is_regressor(self),\n",
    "        )\n",
    "\n",
    "        sample_weight = _check_sample_weight(\n",
    "            sample_weight, X, np.float64, copy=True, only_non_negative=True\n",
    "        )\n",
    "        sample_weight /= sample_weight.sum()\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        # Initialization of the random number instance that will be used to\n",
    "        # generate a seed at each iteration\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        epsilon = np.finfo(sample_weight.dtype).eps\n",
    "\n",
    "        zero_weight_mask = sample_weight == 0.0\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # avoid extremely small sample weight, for details see issue #20320\n",
    "            sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "            # do not clip sample weights that were exactly zero originally\n",
    "            sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost, X, y, sample_weight, random_state\n",
    "            )\n",
    "\n",
    "            # Early termination\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            if not np.isfinite(sample_weight_sum):\n",
    "                warnings.warn(\n",
    "                    \"Sample weights have reached infinite values,\"\n",
    "                    f\" at iteration {iboost}, causing overflow. \"\n",
    "                    \"Iterations stopped. Try lowering the learning rate.\",\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "            # =======  override  =======\n",
    "            sum_n = np.sum(sample_weight[:self.n_protected])\n",
    "            sum_m = np.sum(sample_weight[self.n_protected:])\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize m samples while n are protected\n",
    "                sample_weight[self.n_protected:] *= (1 - sum_n) / sum_m\n",
    "            self.sample_weight = sample_weight\n",
    "            # ==========================\n",
    "        return self\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "\n",
    "        # Weighted sampling of the training set with replacement\n",
    "        bootstrap_idx = random_state.choice(\n",
    "            np.arange(_num_samples(X)),\n",
    "            size=_num_samples(X),\n",
    "            replace=True,\n",
    "            p=sample_weight,\n",
    "        )\n",
    "\n",
    "        # Fit on the bootstrapped sample and obtain a prediction\n",
    "        # for all samples in the training set\n",
    "        X_ = _safe_indexing(X, bootstrap_idx)\n",
    "        y_ = _safe_indexing(y, bootstrap_idx)\n",
    "        estimator.fit(X_, y_)\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        error_vect = np.abs(y_predict - y)\n",
    "        sample_mask = sample_weight > 0\n",
    "\n",
    "        # ========  override  ========\n",
    "        # freeze first n's weight\n",
    "        sample_mask[:self.n_protected].fill(False)\n",
    "        # ============================\n",
    "\n",
    "        masked_sample_weight = sample_weight[sample_mask]\n",
    "        masked_error_vector = error_vect[sample_mask]\n",
    "        error_max = masked_error_vector.max()\n",
    "        if error_max != 0:\n",
    "            masked_error_vector /= error_max\n",
    "\n",
    "        if self.loss == \"square\":\n",
    "            masked_error_vector **= 2\n",
    "        elif self.loss == \"exponential\":\n",
    "            masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "\n",
    "        if estimator_error <= 0:\n",
    "            # Stop if fit is perfect\n",
    "            return sample_weight, 1.0, 0.0\n",
    "\n",
    "        elif estimator_error >= 0.5:\n",
    "            # Discard current estimator only if it isn't the only one\n",
    "            if len(self.estimators_) > 1:\n",
    "                self.estimators_.pop(-1)\n",
    "            return None, None, None\n",
    "\n",
    "        beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "        # Boost weight using AdaBoost.R2 alg\n",
    "        estimator_weight = self.learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            sample_weight[sample_mask] *= np.power(\n",
    "                beta, (1.0 - masked_error_vector) * self.learning_rate\n",
    "            )\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def set_protected(self, n_protected):\n",
    "        self.n_protected = n_protected\n",
    "\n",
    "\n",
    "class TwoStageTrAdaboost:\n",
    "    s = 20  # Hyperparameter S : the number of steps\n",
    "    regs = []  # Regressor saved\n",
    "    learner = None\n",
    "    w = None  # Weight\n",
    "    train_indicator, valid_indicator = {}, {}\n",
    "    verification, display_warning = False, False\n",
    "    n = 0\n",
    "    last_regressor = 0\n",
    "    boost_learner = None\n",
    "    beta_record = []\n",
    "    random_state = 0\n",
    "\n",
    "    def __init__(self, steps: int = 20, base_learner=None, boost_learner=AdaBoostR2T()\n",
    "                 , display_warning: bool = False, random_state=0):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        self.s = steps\n",
    "        self.display_warning = display_warning\n",
    "        if base_learner is None:\n",
    "            self.learner = DecisionTreeRegressor(max_depth=3, random_state=self.random_state)\n",
    "        else:\n",
    "            self.learner = base_learner\n",
    "        self.regs = []\n",
    "        self.w = None\n",
    "        self.train_indicator, self.valid_indicator = {}, {}\n",
    "        self.verification, self.display_warning = False, False\n",
    "        self.n = 0\n",
    "        self.last_regressor = 0\n",
    "        self.boost_learner = boost_learner\n",
    "        self.beta_record = []\n",
    "\n",
    "    def fit(self, x_source: np.ndarray, y_source: np.ndarray, x_target: np.ndarray, y_target: np.ndarray):\n",
    "        from sklearn.metrics import r2_score as r2s\n",
    "        # Initialize\n",
    "        n, m = len(y_source), len(y_target)\n",
    "        self.n = n\n",
    "        self.w = np.zeros((self.s, n + m))\n",
    "        self.w[0, :] = np.ones(n + m) / (n + m)\n",
    "        # Form dataset\n",
    "        x_train = np.concatenate((x_source, x_target), axis=0)\n",
    "        y_train = np.concatenate((y_source, y_target), axis=0)\n",
    "        # Loop\n",
    "        for t in range(self.s):\n",
    "            # Call Adaboost.R2t\n",
    "            r2t = AdaBoostR2T(estimator=self.learner, learning_rate=0.5, n_estimators=20, loss='square',\n",
    "                              random_state=self.random_state)\n",
    "            r2t.set_protected(n)  # freeze first n instances\n",
    "            r2t.fit(x_train, y_train, sample_weight=self.w[t, :])\n",
    "            self.regs.append(r2t)\n",
    "            # Call Learner\n",
    "            learner = self.learner\n",
    "            learner.fit(x_train, y_train, sample_weight=self.w[t, :])\n",
    "            hyp_train = learner.predict(x_train)\n",
    "            # get error\n",
    "            e_learner = np.abs(y_train - hyp_train)\n",
    "            e_t = e_learner / np.max(e_learner)\n",
    "            # Update weight vector\n",
    "            if t == self.s - 1:\n",
    "                break\n",
    "            #   Initialize binary search\n",
    "            beta_0, beta_1 = -20, 1\n",
    "            target_sum = m / (n + m) + t / (self.s - 1) * (1 - m / (n + m))\n",
    "            curr_sum = 10\n",
    "            while abs(target_sum - curr_sum) > 10 ** -3:\n",
    "                curr_beta = beta_0 / 2 + beta_1 / 2\n",
    "                self.w[t + 1, :n] = self.w[t, :n] * np.power(10 ** curr_beta, e_t[:n])\n",
    "                self.w[t + 1, n:] = self.w[t, n:]\n",
    "                self.w[t + 1] /= np.sum(self.w[t + 1])  # Normalize\n",
    "                curr_sum = np.sum(self.w[t + 1, n:])\n",
    "                if curr_sum > target_sum:\n",
    "                    beta_0 = (beta_0 + beta_1) / 2\n",
    "                else:\n",
    "                    beta_1 = (beta_0 + beta_1) / 2\n",
    "                if beta_1 - beta_0 < 10 ** -10:\n",
    "                    if not self.display_warning:\n",
    "                        break\n",
    "                    print('Convergence Failed in Iter {3} with E = {0:.4f}'\n",
    "                          .format(target_sum - curr_sum, beta_0, beta_1, t))\n",
    "                    break\n",
    "            self.beta_record.append(10 ** beta_0)\n",
    "        # Return Train Indicator\n",
    "        #   Initialize\n",
    "        n_4 = len(self.regs)\n",
    "        r2_target, mae_target, mse_target = np.zeros(n_4), np.zeros(n_4), np.zeros(n_4)\n",
    "        #   Calculate\n",
    "        for i in range(n_4):\n",
    "            p_train = self.regs[i].predict(x_train)\n",
    "            r2_target[i] = r2s(y_train[n:], p_train[n:])\n",
    "            mae_target[i] = np.mean(np.abs(y_train[n:] - p_train[n:]))\n",
    "            mse_target[i] = np.mean((y_train[n:] - p_train[n:]) ** 2)\n",
    "        #   Record\n",
    "        self.train_indicator['r2_target'] = r2_target\n",
    "        self.train_indicator['mae_target'] = mae_target\n",
    "        self.train_indicator['mse_target'] = mse_target\n",
    "        # Refine Index\n",
    "        self.verification = False\n",
    "\n",
    "    def valid(self, x_valid: np.ndarray, y_valid: np.ndarray):\n",
    "        from sklearn.metrics import r2_score as r2s\n",
    "        # Return Valid Indicator\n",
    "        #   Initialize\n",
    "        n_4 = len(self.regs)\n",
    "        n = self.n\n",
    "        r2, mae, mse = np.zeros(n_4), np.zeros(n_4), np.zeros(n_4)\n",
    "        #   Calculate\n",
    "        for i in range(n_4):\n",
    "            p_valid = self.regs[i].predict(x_valid)\n",
    "            r2[i] = r2s(y_valid, p_valid)\n",
    "            mae[i] = np.mean(np.abs(y_valid - p_valid))\n",
    "            mse[i] = np.mean((y_valid - p_valid) ** 2)\n",
    "        #   Record\n",
    "        self.valid_indicator['r2'] = r2\n",
    "        self.valid_indicator['mae'] = mae\n",
    "        self.valid_indicator['mse'] = mse\n",
    "        # Refine Index\n",
    "        self.verification = True\n",
    "\n",
    "    def predict(self, x: np.ndarray, principle='mae', valid_indicator=True):\n",
    "        if not self.verification:\n",
    "            raise AssertionError('No Validation Present.')\n",
    "        # Get Regressor\n",
    "        reg_principle = self.valid_indicator.get(principle)\n",
    "        reg_arg = np.argmin(reg_principle)\n",
    "        reg = self.regs[reg_arg]\n",
    "        self.last_regressor = reg_arg\n",
    "        # Predict\n",
    "        return reg.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55fb5f-89cb-4480-9c9f-87eae65379bf",
   "metadata": {},
   "source": [
    "### 0.2.3 Proposed Algorithm ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad16bbd-d8d6-486d-8728-9936436338d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedAlg:\n",
    "    s = 20  # Hyperparameter S : the number of steps\n",
    "    regs = []  # Regressor saved\n",
    "    learner = None\n",
    "    w = None  # Weight\n",
    "    train_indicator, valid_indicator = {}, {}\n",
    "    verification, display_warning = False, False\n",
    "    n = 0\n",
    "    last_regressor = 0\n",
    "    boost_learner = None\n",
    "    beta_record = []\n",
    "    feature_importance = None\n",
    "\n",
    "    def __init__(self, steps: int = 20, base_learner=None, boost_learner=None\n",
    "                 , display_warning: bool = False):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        self.s = steps\n",
    "        self.display_warning = display_warning\n",
    "        if base_learner is None:\n",
    "            self.learner = DecisionTreeRegressor(max_depth=3)\n",
    "        else:\n",
    "            self.learner = base_learner\n",
    "        if boost_learner is None:\n",
    "            self.boost_learner = AdaBoostR2T()\n",
    "        else:\n",
    "            self.boost_learner = boost_learner\n",
    "        self.regs = []\n",
    "        self.w = None\n",
    "        self.train_indicator, self.valid_indicator = {}, {}\n",
    "        self.verification, self.display_warning = False, False\n",
    "        self.n = 0\n",
    "        self.last_regressor = 0\n",
    "        self.boost_learner = boost_learner\n",
    "        self.beta_record = []\n",
    "\n",
    "    def fit(self, x_source: np.ndarray, y_source: np.ndarray, x_target: np.ndarray, y_target: np.ndarray):\n",
    "        from sklearn.metrics import r2_score as r2s\n",
    "        from sklearn.base import clone\n",
    "        # Initialize\n",
    "        n, m = len(y_source), len(y_target)\n",
    "        self.n = n\n",
    "        self.w = np.zeros((self.s, n + m))\n",
    "        self.w[0, :] = np.ones(n + m) / (n + m)\n",
    "        # Form dataset\n",
    "        x_train = np.concatenate((x_source, x_target), axis=0)\n",
    "        y_train = np.concatenate((y_source, y_target), axis=0)\n",
    "        # Loop\n",
    "        for t in range(self.s):\n",
    "            # Call boost_learner\n",
    "            r2t = clone(self.boost_learner)\n",
    "            r2t.fit(x_train, y_train, sample_weight=self.w[t, :])\n",
    "            self.regs.append(r2t)\n",
    "            # Call Learner\n",
    "            learner = clone(self.learner)\n",
    "            learner.fit(x_train, y_train, sample_weight=self.w[t, :])\n",
    "            hyp_train = learner.predict(x_train)\n",
    "            # Cal. error\n",
    "            e_learner = np.abs(y_train - hyp_train)\n",
    "            e_t = e_learner / np.max(e_learner)\n",
    "            # Update weight vector\n",
    "            if t == self.s - 1:\n",
    "                break\n",
    "            # Initialize binary search\n",
    "            beta_0, beta_1 = -20, 1\n",
    "            target_sum = m / (n + m) + t / (self.s - 1) * (1 - m / (n + m))\n",
    "            curr_sum = 10\n",
    "            while abs(target_sum - curr_sum) > 10 ** -3:\n",
    "                curr_beta = beta_0 / 2 + beta_1 / 2\n",
    "                self.w[t + 1, :n] = self.w[t, :n] * np.power(10 ** curr_beta, e_t[:n])\n",
    "                self.w[t + 1, n:] = self.w[t, n:]\n",
    "                self.w[t + 1] /= np.sum(self.w[t + 1])\n",
    "                curr_sum = np.sum(self.w[t + 1, n:])\n",
    "                if curr_sum > target_sum:\n",
    "                    beta_0 = (beta_0 + beta_1) / 2\n",
    "                else:\n",
    "                    beta_1 = (beta_0 + beta_1) / 2\n",
    "                if beta_1 - beta_0 < 10 ** -10:\n",
    "                    if not self.display_warning:\n",
    "                        break\n",
    "                    print('Convergence Failed in Iter {3} with E = {0:.4f}'\n",
    "                          .format(target_sum - curr_sum, beta_0, beta_1, t))\n",
    "                    break\n",
    "            self.beta_record.append(10 ** beta_0)\n",
    "        # Return Train Indicator\n",
    "        #   Initialize\n",
    "        n_4 = len(self.regs)\n",
    "        r2_target, mae_target, mse_target = np.zeros(n_4), np.zeros(n_4), np.zeros(n_4)\n",
    "        #   Calculate\n",
    "        for i in range(n_4):\n",
    "            p_train = self.regs[i].predict(x_train)\n",
    "            r2_target[i] = r2s(y_train[n:], p_train[n:])\n",
    "            mae_target[i] = np.mean(np.abs(y_train[n:] - p_train[n:]))\n",
    "            mse_target[i] = np.mean((y_train[n:] - p_train[n:]) ** 2)\n",
    "        #   Record\n",
    "        self.train_indicator['r2_target'] = r2_target\n",
    "        self.train_indicator['mae_target'] = mae_target\n",
    "        self.train_indicator['mse_target'] = mse_target\n",
    "        # Refine Index\n",
    "        self.verification = False\n",
    "\n",
    "    def predict(self, x: np.ndarray, principle='mae'):\n",
    "        # Get Regressor\n",
    "        reg_principle = self.train_indicator.get(principle + '_target')\n",
    "        reg_arg = reg_principle < np.median(reg_principle)\n",
    "        self.last_regressor = reg_arg\n",
    "        # Predict\n",
    "        predicts = np.zeros([x.shape[0], 1])\n",
    "        for i in range(len(self.regs)):\n",
    "            if reg_arg[i]:\n",
    "                curr_predict = self.regs[i].predict(x)\n",
    "                predicts = np.concatenate([predicts, curr_predict.reshape([curr_predict.shape[0], 1])], axis=1)\n",
    "        predicts = predicts[:, 1:]\n",
    "        return np.mean(predicts, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28142d1f-9814-4c01-a83e-167ff7eb699e",
   "metadata": {},
   "source": [
    "## 0.3 Base Methods for main process ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d3a093-66a8-4df1-831b-d852ae53cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(d_path: str, label_col_str: str, feature_col_str: str = 'B', c_index: bool = False):\n",
    "    import pandas as pd\n",
    "    # get column index\n",
    "    label_col, count = 0, 0\n",
    "    for i in label_col_str:\n",
    "        count += 1\n",
    "        if len(label_col_str) - count == 1:\n",
    "            label_col += (ord(i.lower()) - 96) * 26\n",
    "        else:\n",
    "            label_col += (ord(i.lower()) - 96)\n",
    "    feature_col, count = 0, 0\n",
    "    for i in feature_col_str:\n",
    "        count += 1\n",
    "        if len(feature_col_str) - count == 1:\n",
    "            feature_col += (ord(i.lower()) - 96) * 26\n",
    "        else:\n",
    "            feature_col += (ord(i.lower()) - 96)\n",
    "    # get data\n",
    "    db = pd.read_excel(d_path)\n",
    "    x = db.iloc[:, feature_col - 1:label_col - 1]\n",
    "    y = db.iloc[:, label_col - 1]\n",
    "    x_index = x.columns.to_list()\n",
    "    y_index = y.name\n",
    "    x = np.array(x, dtype='float64')\n",
    "    y = np.array(y, dtype='float64')\n",
    "    if c_index:\n",
    "        return x, y, x_index, y_index\n",
    "    else:\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca029ea-20a8-4bce-9e49-ea81ece40d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_line(d_path: str, col_index: str):\n",
    "    import pandas as pd\n",
    "    label_col, count = 0, 0\n",
    "    for i in col_index:\n",
    "        count += 1\n",
    "        if len(col_index) - count == 1:\n",
    "            label_col += (ord(i.lower()) - 96) * 26\n",
    "        else:\n",
    "            label_col += (ord(i.lower()) - 96)\n",
    "    db = pd.read_excel(d_path)\n",
    "    y = db.iloc[:, label_col - 1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85ae5ca-8c9f-4b2a-b615-e097508343d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subplot_id(n_row: int, n_col: int, curr_id: int):\n",
    "    curr_row = 0\n",
    "    while curr_id >= n_row:\n",
    "        curr_row += 1\n",
    "        curr_id -= n_row\n",
    "    return curr_row, curr_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25edae78-a509-42a1-a882-a79d67743b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lists(a: list, b: list, reverse=True):\n",
    "    d = dict(zip(a, b))\n",
    "    a_sorted = sorted(d, reverse=reverse)\n",
    "    b_sorted = []\n",
    "    for i in a_sorted:\n",
    "        b_sorted.append(d.get(i))\n",
    "    return a_sorted, b_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e4476f-8f02-4762-bdc8-d95ca6415b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a20_index(true_values: np.ndarray, predict_values: np.ndarray, variation: float = 0.2) -> float:\n",
    "    m20 = 0\n",
    "    for ii, jj in zip(true_values, predict_values):\n",
    "        if jj * (1-variation) <= ii <= jj * (1+variation):\n",
    "            m20 += 1\n",
    "    return m20 / true_values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55219ae6-01eb-43b1-b651-94ce92b19331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_bias(true_values: np.ndarray, predict_values: np.ndarray) -> float:\n",
    "    a_sub_p = true_values - predict_values\n",
    "    a_add_p = true_values + predict_values\n",
    "    return 2 * a_sub_p.sum() / a_add_p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350652a9-7f17-442c-bb61-7180394c76b3",
   "metadata": {},
   "source": [
    "# 1 Import Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc3f34b-3c9e-444d-8c7f-238434d9af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original_tar, y_tar, x_name, y_name = load_data(path_tar, col_tar, c_index=True)\n",
    "x_original_s, ys = load_data(path_sou, col_sou)\n",
    "ids = load_data_line(path_tar, 'A').tolist()\n",
    "feature_names = ['$A_{core}$', \"$f_{co}'$\", '$d_t$', '$s_t$', '$E_t$', '$f_{fu}$', '$Shape$']\n",
    "feature_units = ['(mm$^2$)', '(MPa)', '(mm)', '(mm)', '(GPa)', '(MPa)', '']\n",
    "label_name = '$f_{cc}\\'$'\n",
    "reg_names = ['RF', 'Extra Trees', 'AdaBoost.R2', 'XGBoost',\n",
    "             'TrAdaBoost.R2', 'Two-stage TrAdaBoost.R2', 'proposed model']\n",
    "reg_names_simplified = ['RF', 'ET', 'AD', 'XG', 'TA', 'TS', 'PM']\n",
    "metric_names = ['R2TR', 'R2', 'RMSE', 'MAE', 'MAPE', 'A20', 'FB']\n",
    "rec_space = list(product(metric_names, reg_names_simplified))\n",
    "rec_cols = ['RS']\n",
    "rec_cols.extend([i[1] + '_' + i[0] for i in rec_space])\n",
    "df = pd.DataFrame(columns=rec_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6298bc5-0651-407a-8359-7ac9fcd3d992",
   "metadata": {},
   "source": [
    "# 2 Monte Carlo Simulation #"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25c09203-d100-46eb-97e2-d5b8fe94d782",
   "metadata": {},
   "source": [
    "Note that Monte Carlo simulation is very time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b10882-ff8a-4f33-a0c9-561816ee7842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1500/1500 [5:13:16<00:00, 12.53s/it]\n"
     ]
    }
   ],
   "source": [
    "rs_initial, rs_steps = 0, 1500\n",
    "for random_state in trange(rs_initial, rs_steps):\n",
    "    # 2.1 Define Models\n",
    "    np.random.seed(random_state)\n",
    "    learner = RandomForestRegressor(n_estimators=200, max_features=7, max_depth=12,\n",
    "                                    criterion='absolute_error', random_state=random_state)\n",
    "    base_learner = DecisionTreeRegressor(max_depth=15, criterion='friedman_mse', random_state=random_state)\n",
    "    regs = [RandomForestRegressor(n_estimators=200, max_features=7, max_depth=11,\n",
    "                                  criterion='poisson', random_state=random_state),\n",
    "            ExtraTreesRegressor(n_estimators=200, max_features=6, max_depth=5,\n",
    "                                criterion='poisson', random_state=random_state),\n",
    "            AdaBoostRegressor(n_estimators=200, learning_rate=0.7740, loss='linear', random_state=random_state),\n",
    "            xgboost.XGBRegressor(n_estimators=200, max_depth=15, learning_rate=1.5583,\n",
    "                                 reg_alpha=21.469, reg_lambda=446.24),\n",
    "            #  Transfer\n",
    "            TrAdaBoostR(base_learner),\n",
    "            TwoStageTrAdaboost(),\n",
    "            ProposedAlg(base_learner=base_learner, boost_learner=learner)\n",
    "            ]\n",
    "    # 2.2 Normalization\n",
    "    sc = Scaler()\n",
    "    sc.fit(x_original_tar)\n",
    "    x_tar, xs = sc.transform(x_original_tar), sc.transform(x_original_s)\n",
    "    # 2.3 Train-Test set spilt\n",
    "    xt, x_test, yt, y_test = train_test_split(x_tar, y_tar, test_size=0.2, random_state=random_state)\n",
    "    ids_train, ids_test, _, _ = train_test_split(ids, y_tar, test_size=0.2, random_state=random_state)\n",
    "    # 2.4 Model Training\n",
    "    p_trains, p_tests = [], []\n",
    "    metrics = []\n",
    "    for i in range(len(regs)):\n",
    "        if regs[i] is None:\n",
    "            continue\n",
    "        reg = regs[i]\n",
    "        if i <= 3:  # 0, 1, 2, 3\n",
    "            reg.fit(xt, yt)\n",
    "        elif i == 5:  # 5\n",
    "            x_train, x_valid, y_train, y_valid = train_test_split(xt, yt,\n",
    "                                                                  test_size=0.2, random_state=random_state)\n",
    "            reg.fit(xs, ys, x_train, y_train)\n",
    "            reg.valid(x_valid, y_valid)\n",
    "        else:  # 4, 6\n",
    "            reg.fit(xs, ys, xt, yt)\n",
    "        p_train, p_test = reg.predict(xt), reg.predict(x_test)\n",
    "        p_trains.append(p_train)\n",
    "        p_tests.append(p_test)\n",
    "        # Metrics\n",
    "        metric = [r2s(yt, p_train),\n",
    "                  r2s(y_test, p_test),\n",
    "                  mean_squared_error(y_test, p_test) ** 0.5,\n",
    "                  mean_absolute_error(y_test, p_test),\n",
    "                  mean_absolute_percentage_error(y_test, p_test),\n",
    "                  a20_index(y_test, p_test),\n",
    "                  fractional_bias(y_test, p_test)]\n",
    "        metrics.append(metric)\n",
    "    metrics_flatten = [random_state]\n",
    "    for i in range(len(metrics)):\n",
    "        for j in range(len(reg_names)):\n",
    "            metrics_flatten.append(metrics[j][i])\n",
    "    df.loc[len(df)] = metrics_flatten\n",
    "    # 2.5 Record to Excel\n",
    "df.to_excel(save_path + 'RS = {0} to {1}.xlsx'.format(rs_initial, rs_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46b1b7-3740-4464-8f15-24d02e47e08a",
   "metadata": {},
   "source": [
    "# 3 Get Metrics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bc53d4-5194-4012-b81d-46e2c749f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_results_path = save_path + 'RS = {0} to {1}.xlsx'.format(rs_initial, rs_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3fae4c-c4fe-459f-8daa-8b3282b40443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Mean', 'Left CI', 'Right CI'])\n",
    "r2 = ['J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "\n",
    "for i in r2:\n",
    "    pm_r2 = np.array(load_data_line(mc_results_path, i))\n",
    "    pm_r2_sorted, pm_length = np.sort(pm_r2), pm_r2.shape[0]\n",
    "    left_index, right_index = int(pm_length * 2.5 / 100), int(pm_length * 97.5 / 100)\n",
    "    confidence = [pm_r2_sorted.mean(), pm_r2_sorted[left_index], pm_r2_sorted[right_index]]\n",
    "    df.loc[len(df)] = confidence\n",
    "df = df.rename(index=dict(zip(list(range(7)), reg_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2eb24d0-57c2-4caa-87e2-091035ef44d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Left CI</th>\n",
       "      <th>Right CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.813155</td>\n",
       "      <td>0.612408</td>\n",
       "      <td>0.925509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.817259</td>\n",
       "      <td>0.615170</td>\n",
       "      <td>0.923669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost.R2</th>\n",
       "      <td>0.793693</td>\n",
       "      <td>0.584316</td>\n",
       "      <td>0.906290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.598201</td>\n",
       "      <td>0.904228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrAdaBoost.R2</th>\n",
       "      <td>0.791583</td>\n",
       "      <td>0.547868</td>\n",
       "      <td>0.914719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two-stage TrAdaBoost.R2</th>\n",
       "      <td>0.731366</td>\n",
       "      <td>0.460023</td>\n",
       "      <td>0.885549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposed model</th>\n",
       "      <td>0.826906</td>\n",
       "      <td>0.652269</td>\n",
       "      <td>0.923808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Mean   Left CI  Right CI\n",
       "RF                       0.813155  0.612408  0.925509\n",
       "Extra Trees              0.817259  0.615170  0.923669\n",
       "AdaBoost.R2              0.793693  0.584316  0.906290\n",
       "XGBoost                  0.794286  0.598201  0.904228\n",
       "TrAdaBoost.R2            0.791583  0.547868  0.914719\n",
       "Two-stage TrAdaBoost.R2  0.731366  0.460023  0.885549\n",
       "proposed model           0.826906  0.652269  0.923808"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24deddb-5f68-4dca-9929-ba74b9650c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Left CI</th>\n",
       "      <th>Right CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.9255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.8173</td>\n",
       "      <td>0.6152</td>\n",
       "      <td>0.9237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost.R2</th>\n",
       "      <td>0.7937</td>\n",
       "      <td>0.5843</td>\n",
       "      <td>0.9063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.7943</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrAdaBoost.R2</th>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.5479</td>\n",
       "      <td>0.9147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two-stage TrAdaBoost.R2</th>\n",
       "      <td>0.7314</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.8855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposed model</th>\n",
       "      <td>0.8269</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.9238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Mean  Left CI  Right CI\n",
       "RF                       0.8132   0.6124    0.9255\n",
       "Extra Trees              0.8173   0.6152    0.9237\n",
       "AdaBoost.R2              0.7937   0.5843    0.9063\n",
       "XGBoost                  0.7943   0.5982    0.9042\n",
       "TrAdaBoost.R2            0.7916   0.5479    0.9147\n",
       "Two-stage TrAdaBoost.R2  0.7314   0.4600    0.8855\n",
       "proposed model           0.8269   0.6523    0.9238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fe5fb-367e-4752-9d20-fb17218d3bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
